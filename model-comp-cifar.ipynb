{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Model Comparison Jupyter notebook\r\n",
    "\r\n",
    "This notebook will load nets as defined int the imported python modules with certain preformance characteristics for comparison.\r\n",
    "This file will also contain the tensorboard that helps visualize the models accuracy over time and the training process.\r\n",
    "\r\n",
    "Components:\r\n",
    "- initialize training parameters and fetch dataset (including RotMNIST dataset)\r\n",
    "- compare model attributes (such as total parameters and structure)\r\n",
    "- define hyperparameters\r\n",
    "- create tensorboard and set up preformance graphs\r\n",
    "- compare model training and preformance under different conditions\r\n",
    "- save models to files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "### Imports for pytorch and dataset\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch.autograd import Variable\r\n",
    "\r\n",
    "import torchvision\r\n",
    "from torchvision import datasets\r\n",
    "import torchvision.transforms as transforms\r\n",
    "\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "from torchvision.datasets.cifar import CIFAR10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "### Defining simple composition for rotations for following compose\r\n",
    "import torchvision.transforms.functional as TF\r\n",
    "import random\r\n",
    "\r\n",
    "class RotationP4:\r\n",
    "    \"\"\"Rotate randomly in p4 group\"\"\"\r\n",
    "\r\n",
    "    def __init__(self):\r\n",
    "        self.angles = [-90, 0, 90, 180]\r\n",
    "        \r\n",
    "    def __call__(self, x):\r\n",
    "        angle = random.choice(self.angles)\r\n",
    "        return TF.rotate(x, angle)\r\n",
    "\r\n",
    "augmented_transform = transforms.Compose(\r\n",
    "    [transforms.RandomHorizontalFlip(),\r\n",
    "    transforms.ToTensor(),\r\n",
    "    RotationP4(),\r\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\r\n",
    "\r\n",
    "normal_transforms = transform_train = transforms.Compose(\r\n",
    "    [transforms.ToTensor(),\r\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "### Define dataloaders\r\n",
    "### Instatiate RotMNIST and verify behaviour below with the dataloaders\r\n",
    "dataset_rot = CIFAR10(\r\n",
    "    root = 'data',\r\n",
    "    download=True,\r\n",
    "    train=True,\r\n",
    "    transform=augmented_transform\r\n",
    ")\r\n",
    "\r\n",
    "test_dataset_rot = CIFAR10(\r\n",
    "    root = 'data',\r\n",
    "    download=True,\r\n",
    "    train=False,\r\n",
    "    transform=augmented_transform\r\n",
    ")\r\n",
    "\r\n",
    "dataset_upright = CIFAR10(\r\n",
    "    root = 'data',\r\n",
    "    download=True,\r\n",
    "    train=True,\r\n",
    "    transform=normal_transforms,\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "### Import different networks from python files\r\n",
    "# TODO: Uncomment other networks and import\r\n",
    "from p4m_conv import P4MNetC\r\n",
    "from p4_conv import P4NetC\r\n",
    "from z2_conv import ConvNetC\r\n",
    "\r\n",
    "p4m_net = P4MNetC()\r\n",
    "p4_net = P4NetC()\r\n",
    "conv_net = ConvNetC()\r\n",
    "\r\n",
    "p4m_total_params = sum(p.numel() for p in p4m_net.parameters() if p.requires_grad)\r\n",
    "p4_total_params = sum(p.numel() for p in p4_net.parameters() if p.requires_grad)\r\n",
    "z2_total_params = sum(p.numel() for p in conv_net.parameters() if p.requires_grad)\r\n",
    "\r\n",
    "print(p4m_net)\r\n",
    "print(p4_net)\r\n",
    "print(conv_net)\r\n",
    "\r\n",
    "print(\"P4M  --\\tTrainable Params: \" + str(p4m_total_params))\r\n",
    "print(\"P4   --\\tTrainable Params: \" + str(p4_total_params))\r\n",
    "print(\"Conv --\\tTrainable Params: \" + str(z2_total_params))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "P4MNetC(\n",
      "  (conv1): P4MConvZ2()\n",
      "  (conv2): P4MConvP4M()\n",
      "  (conv3): P4MConvP4M()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "P4NetC(\n",
      "  (conv1): P4ConvZ2()\n",
      "  (conv2): P4ConvP4()\n",
      "  (conv3): P4ConvP4()\n",
      "  (fc1): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "ConvNetC(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1152, out_features=184, bias=True)\n",
      "  (fc2): Linear(in_features=184, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "P4M  --\tTrainable Params: 515722\n",
      "P4   --\tTrainable Params: 68018\n",
      "Conv --\tTrainable Params: 229730\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "### Hyperparameters\r\n",
    "learning_rate = 0.001\r\n",
    "batch_size = 64\r\n",
    "epochs = 25\r\n",
    "\r\n",
    "### Objectives/Loss fn\r\n",
    "loss_fn = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "### Dataloaders\r\n",
    "train_dataloader_rot = DataLoader(dataset_rot, batch_size=batch_size, shuffle=True)\r\n",
    "test_dataloader_rot = DataLoader(test_dataset_rot, batch_size=batch_size, shuffle=True)\r\n",
    "train_dataloader_upright = DataLoader(dataset_upright, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, cur_epoch):\r\n",
    "    running_loss = 0.0\r\n",
    "    size = len(dataloader.dataset)\r\n",
    "    for batch, (X, y) in enumerate(dataloader):\r\n",
    "        model.to(device)\r\n",
    "        # Compute prediction and loss for backprop\r\n",
    "        pred = model(X.to(device))\r\n",
    "        loss = loss_fn(pred, y.to(device))\r\n",
    "\r\n",
    "        # Backpropagation by setting grad to zero, calculating using backprop engine and stepping (using learning rate)\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        running_loss += loss.item()\r\n",
    "\r\n",
    "        if batch % 100 == 99:\r\n",
    "            loss, current = loss.item(), batch * len(X)\r\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\r\n",
    "\r\n",
    "            writer.add_scalar('training loss', running_loss / 100, cur_epoch * len(dataloader) + batch)\r\n",
    "            running_loss = 0.0\r\n",
    "\r\n",
    "def test_loop(dataloader, model, loss_fn):\r\n",
    "    model.to(device)\r\n",
    "    size = len(dataloader.dataset)\r\n",
    "    test_loss, correct = 0, 0\r\n",
    "    \r\n",
    "    # No gradient on training data (faster computation and no optimization happening here anyway)\r\n",
    "    with torch.no_grad():\r\n",
    "        for X, y in dataloader:\r\n",
    "            pred = model(X.to(device))\r\n",
    "            test_loss += loss_fn(pred, y.to(device)).item()\r\n",
    "            correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\r\n",
    "\r\n",
    "    test_loss /= size\r\n",
    "    correct /= size\r\n",
    "    \r\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\r\n",
    "    return correct"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "### Train all networks\r\n",
    "def train_test_net(net, train_upright):\r\n",
    "    # Add option to train networks with RotMNIST\r\n",
    "    test_dataloader = train_dataloader_rot\r\n",
    "    if (train_upright):\r\n",
    "        test_dataloader = train_dataloader_upright\r\n",
    "\r\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\r\n",
    "    for t in range(epochs):\r\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\r\n",
    "        train_loop(test_dataloader, net, loss_fn, optimizer, t)\r\n",
    "        correct = test_loop(test_dataloader_rot, net, loss_fn)\r\n",
    "        writer.add_scalar('Test Performance', correct, t * len(test_dataloader_rot) + batch_size)\r\n",
    "    print('Finished Training Net + ' + str(type(net)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "\r\n",
    "import torch.utils.tensorboard\r\n",
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "\r\n",
    "# writer = SummaryWriter('runs/p4m_CIFAR_1')\r\n",
    "# train_test_net(p4m_net, True)\r\n",
    "\r\n",
    "writer = SummaryWriter('runs/p4_CIFAR_1')\r\n",
    "train_test_net(p4_net, True)\r\n",
    "\r\n",
    "writer = SummaryWriter('runs/conv_CIFAR_1')\r\n",
    "train_test_net(conv_net, False)\r\n",
    "\r\n",
    "convu_net = ConvNetC()\r\n",
    "writer = SummaryWriter('runs/convu_CIFAR_1')\r\n",
    "train_test_net(convu_net, True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.264875  [ 6336/50000]\n",
      "loss: 2.252113  [12736/50000]\n",
      "loss: 2.255557  [19136/50000]\n",
      "loss: 2.127886  [25536/50000]\n",
      "loss: 2.150359  [31936/50000]\n",
      "loss: 2.009891  [38336/50000]\n",
      "loss: 2.126639  [44736/50000]\n",
      "Test Error: \n",
      " Accuracy: 21.6%, Avg loss: 0.032397 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.935604  [ 6336/50000]\n",
      "loss: 1.841633  [12736/50000]\n",
      "loss: 1.971152  [19136/50000]\n",
      "loss: 2.091546  [25536/50000]\n",
      "loss: 2.088402  [31936/50000]\n",
      "loss: 1.979169  [38336/50000]\n",
      "loss: 1.981676  [44736/50000]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 0.030149 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.934158  [ 6336/50000]\n",
      "loss: 1.832450  [12736/50000]\n",
      "loss: 1.828944  [19136/50000]\n",
      "loss: 1.804041  [25536/50000]\n",
      "loss: 2.065847  [31936/50000]\n",
      "loss: 1.838399  [38336/50000]\n",
      "loss: 1.788660  [44736/50000]\n",
      "Test Error: \n",
      " Accuracy: 31.3%, Avg loss: 0.029093 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.760550  [ 6336/50000]\n",
      "loss: 1.747302  [12736/50000]\n",
      "loss: 1.715676  [19136/50000]\n",
      "loss: 1.798766  [25536/50000]\n",
      "loss: 1.831088  [31936/50000]\n",
      "loss: 1.687186  [38336/50000]\n",
      "loss: 1.729650  [44736/50000]\n",
      "Test Error: \n",
      " Accuracy: 36.0%, Avg loss: 0.027097 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.784950  [ 6336/50000]\n",
      "loss: 1.495345  [12736/50000]\n",
      "loss: 1.881525  [19136/50000]\n",
      "loss: 1.769887  [25536/50000]\n",
      "loss: 1.760222  [31936/50000]\n",
      "loss: 1.776795  [38336/50000]\n",
      "loss: 1.514719  [44736/50000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 0.026147 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.699222  [ 6336/50000]\n",
      "loss: 1.581581  [12736/50000]\n",
      "loss: 1.599168  [19136/50000]\n",
      "loss: 1.461867  [25536/50000]\n",
      "loss: 1.515351  [31936/50000]\n",
      "loss: 1.580811  [38336/50000]\n",
      "loss: 1.488931  [44736/50000]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 0.024777 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.674429  [ 6336/50000]\n",
      "loss: 1.611942  [12736/50000]\n",
      "loss: 1.417126  [19136/50000]\n",
      "loss: 1.519007  [25536/50000]\n",
      "loss: 1.838911  [31936/50000]\n",
      "loss: 1.621971  [38336/50000]\n",
      "loss: 1.460134  [44736/50000]\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 0.023700 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.364525  [ 6336/50000]\n",
      "loss: 1.274946  [12736/50000]\n",
      "loss: 1.432640  [19136/50000]\n",
      "loss: 1.413711  [25536/50000]\n",
      "loss: 1.406047  [31936/50000]\n",
      "loss: 1.450386  [38336/50000]\n",
      "loss: 1.650994  [44736/50000]\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 0.022869 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.363231  [ 6336/50000]\n",
      "loss: 1.535393  [12736/50000]\n",
      "loss: 1.440647  [19136/50000]\n",
      "loss: 1.629623  [25536/50000]\n",
      "loss: 1.245864  [31936/50000]\n",
      "loss: 1.274333  [38336/50000]\n",
      "loss: 1.239919  [44736/50000]\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 0.022245 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.456824  [ 6336/50000]\n",
      "loss: 1.232447  [12736/50000]\n",
      "loss: 1.161119  [19136/50000]\n",
      "loss: 1.451026  [25536/50000]\n",
      "loss: 1.389038  [31936/50000]\n",
      "loss: 1.446365  [38336/50000]\n",
      "loss: 1.346693  [44736/50000]\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 0.021656 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.269993  [ 6336/50000]\n",
      "loss: 1.252090  [12736/50000]\n",
      "loss: 1.390700  [19136/50000]\n",
      "loss: 1.111625  [25536/50000]\n",
      "loss: 1.287247  [31936/50000]\n",
      "loss: 1.015513  [38336/50000]\n",
      "loss: 1.387024  [44736/50000]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 0.021474 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.347350  [ 6336/50000]\n",
      "loss: 1.026274  [12736/50000]\n",
      "loss: 1.376132  [19136/50000]\n",
      "loss: 1.105083  [25536/50000]\n",
      "loss: 1.190776  [31936/50000]\n",
      "loss: 0.928626  [38336/50000]\n",
      "loss: 1.217339  [44736/50000]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 0.020579 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.227484  [ 6336/50000]\n",
      "loss: 1.446860  [12736/50000]\n",
      "loss: 1.038933  [19136/50000]\n",
      "loss: 1.297392  [25536/50000]\n",
      "loss: 1.210905  [31936/50000]\n",
      "loss: 1.161181  [38336/50000]\n",
      "loss: 1.292090  [44736/50000]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 0.020350 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b220b30f809a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'runs/p4_CIFAR_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_test_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp4_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'runs/conv_CIFAR_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-edd1c35f5a5b>\u001b[0m in \u001b[0;36mtrain_test_net\u001b[1;34m(net, train_upright)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader_rot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test Performance'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader_rot\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-f7f31add6ebe>\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, cur_epoch)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# Backpropagation by setting grad to zero, calculating using backprop engine and stepping (using learning rate)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\reu-code\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\reu-code\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Comparisons\r\n",
    "\r\n",
    "There are a few comparisons between the models to be made here. Here is a list of the following that I log\r\n",
    "- Model accuracy on 10000 test images\r\n",
    "- Model accuracy per class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_accuracy(net):\r\n",
    "    correct = 0\r\n",
    "    total = 0\r\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\r\n",
    "    with torch.no_grad():\r\n",
    "        for data in test_dataloader_rot:\r\n",
    "            images, labels = data[0].to(device), data[1].to(device)\r\n",
    "            # calculate outputs by running images through the network\r\n",
    "            outputs = net(images)\r\n",
    "            # the class with the highest energy is what we choose as prediction\r\n",
    "            _, predicted = torch.max(outputs.data, 1)\r\n",
    "            total += labels.size(0)\r\n",
    "            correct += (predicted == labels.to(device)).sum().item()\r\n",
    "\r\n",
    "    print('Accuracy of the ' + str(type(net)) + ' on the 10000 test images: %f %%' % (\r\n",
    "        100.0 * correct / total))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def class_labels(net):\r\n",
    "    # prepare to count predictions for each class\r\n",
    "    correct_pred = {num : 0 for num in range(0, 10)}\r\n",
    "    total_pred = {num : 0 for num in range(0, 10)}\r\n",
    "\r\n",
    "    # again no gradients needed\r\n",
    "    with torch.no_grad():\r\n",
    "        for data in test_dataloader_rot:\r\n",
    "            images, labels = data[0].to(device), data[1].to(device)\r\n",
    "            outputs = net(images.to(device))\r\n",
    "            _, predictions = torch.max(outputs, 1)\r\n",
    "            # collect the correct predictions for each class\r\n",
    "            for label, prediction in zip(labels, predictions):\r\n",
    "                if label == prediction:\r\n",
    "                    correct_pred[label.item()] += 1\r\n",
    "                total_pred[label.item()] += 1\r\n",
    "\r\n",
    "    print('Classes for ' + str(type(net)))\r\n",
    "\r\n",
    "    # print accuracy for each class\r\n",
    "    for classname, correct_count in correct_pred.items():\r\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\r\n",
    "        print(\"Accuracy for num {} is: {:.1f} %\".format(classname,\r\n",
    "                                                   accuracy))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_accuracy(p4m_net)\r\n",
    "class_labels(p4m_net)\r\n",
    "\r\n",
    "test_accuracy(p4_net)\r\n",
    "class_labels(p4_net)\r\n",
    "\r\n",
    "test_accuracy(conv_net)\r\n",
    "class_labels(conv_net)\r\n",
    "\r\n",
    "test_accuracy(convu_net)\r\n",
    "class_labels(convu_net)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## TODO: Uncomment\r\n",
    "torch.save(p4m_net, 'upright-trained-p4m-cifar-1.pth')\r\n",
    "torch.save(p4_net, 'upright-trained-p4-cifar-1.pth')\r\n",
    "torch.save(conv_net, 'rot-trained-conv-cifar-1.pth')\r\n",
    "torch.save(convu_net, 'upright-trained-conv-cifar-1.pth')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f665fd8ed1386b9605bd6d1d95408943e5396eca0f77e44c2585e6a9876cbe3c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('reu-code': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}