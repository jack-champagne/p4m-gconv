{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('reu-code': conda)"
  },
  "interpreter": {
   "hash": "f665fd8ed1386b9605bd6d1d95408943e5396eca0f77e44c2585e6a9876cbe3c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports for pytorch\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from groupy.gconv.pytorch_gconv import P4MConvP4M\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 8, 30, 1])\n"
     ]
    }
   ],
   "source": [
    "### Convolutional construction. See github repo @ COGMAR/RotEqNet\n",
    "\n",
    "# Construct G-Conv layers\n",
    "C1 = P4MConvP4M(in_channels=1, out_channels=1, kernel_size=5, stride=1, padding=1)\n",
    "\n",
    "# Test code for tensor dimensions\n",
    "# Create 10 images with 1 channels and 32x32 pixels:\n",
    "img = torch.ones([10, 8, 32, 3])\n",
    "x = Variable(img)\n",
    "\n",
    "# fprop\n",
    "y = C1(x)\n",
    "print(y.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define max pooling as from \n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def plane_group_spatial_max_pooling(x, ksize, stride=None, pad=0):\n",
    "#     xs = x.size()\n",
    "#     x = x.view(xs[0], xs[1] * xs[2], xs[3], xs[4])\n",
    "#     x = F.max_pool2d(input=x, kernel_size=ksize, stride=stride, padding=pad)\n",
    "#     x = x.view(xs[0], xs[1], xs[2], x.size()[2], x.size()[3])\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convolution construction code for groupy\n",
    "\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import Parameter\n",
    "# import torch.nn.functional as F\n",
    "# import math\n",
    "# from torch.nn.modules.utils import _pair\n",
    "# from groupy.gconv.make_gconv_indices import *\n",
    "\n",
    "# make_indices_functions = {(1, 4): make_c4_z2_indices,\n",
    "#                           (4, 4): make_c4_p4_indices,\n",
    "#                           (1, 8): make_d4_z2_indices,\n",
    "#                           (8, 8): make_d4_p4m_indices}\n",
    "\n",
    "\n",
    "# def trans_filter(w, inds):\n",
    "#     inds_reshape = inds.reshape((-1, inds.shape[-1])).astype(np.int64)\n",
    "#     w_indexed = w[:, :, inds_reshape[:, 0].tolist(), inds_reshape[:, 1].tolist(), inds_reshape[:, 2].tolist()]\n",
    "#     w_indexed = w_indexed.view(w_indexed.size()[0], w_indexed.size()[1],\n",
    "#                                     inds.shape[0], inds.shape[1], inds.shape[2], inds.shape[3])\n",
    "#     w_transformed = w_indexed.permute(0, 2, 1, 3, 4, 5)\n",
    "#     return w_transformed.contiguous()\n",
    "\n",
    "\n",
    "# class SplitGConv2D(nn.Module):\n",
    "\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "#                  padding=0, bias=True, input_stabilizer_size=1, output_stabilizer_size=4):\n",
    "#         super(SplitGConv2D, self).__init__()\n",
    "#         assert (input_stabilizer_size, output_stabilizer_size) in make_indices_functions.keys()\n",
    "#         self.ksize = kernel_size\n",
    "\n",
    "#         kernel_size = _pair(kernel_size)\n",
    "#         stride = _pair(stride)\n",
    "#         padding = _pair(padding)\n",
    "\n",
    "#         self.in_channels = in_channels\n",
    "#         self.out_channels = out_channels\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.stride = stride\n",
    "#         self.padding = padding\n",
    "#         self.input_stabilizer_size = input_stabilizer_size\n",
    "#         self.output_stabilizer_size = output_stabilizer_size\n",
    "\n",
    "#         self.weight = Parameter(torch.Tensor(\n",
    "#             out_channels, in_channels, self.input_stabilizer_size, *kernel_size))\n",
    "#         if bias:\n",
    "#             self.bias = Parameter(torch.Tensor(out_channels))\n",
    "#         else:\n",
    "#             self.register_parameter('bias', None)\n",
    "#         self.reset_parameters()\n",
    "\n",
    "#         self.inds = self.make_transformation_indices()\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         n = self.in_channels\n",
    "#         for k in self.kernel_size:\n",
    "#             n *= k\n",
    "#         stdv = 1. / math.sqrt(n)\n",
    "#         self.weight.data.uniform_(-stdv, stdv)\n",
    "#         if self.bias is not None:\n",
    "#             self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "#     def make_transformation_indices(self):\n",
    "#         return make_indices_functions[(self.input_stabilizer_size, self.output_stabilizer_size)](self.ksize)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         tw = trans_filter(self.weight, self.inds)\n",
    "#         tw_shape = (self.out_channels * self.output_stabilizer_size,\n",
    "#                     self.in_channels * self.input_stabilizer_size,\n",
    "#                     self.ksize, self.ksize)\n",
    "#         tw = tw.view(tw_shape)\n",
    "\n",
    "#         input_shape = input.size()\n",
    "#         input = input.view(input_shape[0], self.in_channels*self.input_stabilizer_size, input_shape[-2], input_shape[-1])\n",
    "\n",
    "#         y = F.conv2d(input, weight=tw, bias=None, stride=self.stride,\n",
    "#                         padding=self.padding)\n",
    "#         batch_size, _, ny_out, nx_out = y.size()\n",
    "#         y = y.view(batch_size, self.out_channels, self.output_stabilizer_size, ny_out, nx_out)\n",
    "\n",
    "#         if self.bias is not None:\n",
    "#             bias = self.bias.view(1, self.out_channels, 1, 1, 1)\n",
    "#             y = y + bias\n",
    "\n",
    "#         return y\n",
    "\n",
    "\n",
    "# class P4ConvZ2(SplitGConv2D):\n",
    "\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super(P4ConvZ2, self).__init__(input_stabilizer_size=1, output_stabilizer_size=4, *args, **kwargs)\n",
    "\n",
    "\n",
    "# class P4ConvP4(SplitGConv2D):\n",
    "\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super(P4ConvP4, self).__init__(input_stabilizer_size=4, output_stabilizer_size=4, *args, **kwargs)\n",
    "\n",
    "\n",
    "# class P4MConvZ2(SplitGConv2D):\n",
    "\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super(P4MConvZ2, self).__init__(input_stabilizer_size=1, output_stabilizer_size=8, *args, **kwargs)\n",
    "\n",
    "\n",
    "# class P4MConvP4M(SplitGConv2D):\n",
    "\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super(P4MConvP4M, self).__init__(input_stabilizer_size=8, output_stabilizer_size=8, *args, **kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download MNIST\n",
    "\n",
    "# temp_dataset = torchvision.datasets.MNIST(\n",
    "#     root = 'data',\n",
    "#     download=True,\n",
    "#     train=True,\n",
    "#     transform=torchvision.transforms.Compose(\n",
    "#         [torchvision.transforms.Resize(32), torchvision.transforms.ToTensor()]\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test plotting of MNIST\n",
    "\n",
    "# figure = plt.figure(figsize=(8, 8))\n",
    "# sample_idx = torch.randint(len(dataset), size=(1,)).item()\n",
    "# img, label = dataset[sample_idx]\n",
    "# plt.title(label)\n",
    "# plt.axis(\"off\")\n",
    "# plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pytorch dataloader/dataset (RotMNIST)\n",
    "\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import warnings\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import torch\n",
    "import codecs\n",
    "import string\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from urllib.error import URLError\n",
    "from torchvision.datasets.utils import download_and_extract_archive, extract_archive, verify_str_arg, check_integrity\n",
    "import shutil\n",
    "import torchvision.datasets.mnist as mts\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RotMNIST dataset derived from VisionDataset \n",
    "# (Probably could derive it from MNIST instead of just VisionDataset)\n",
    "\n",
    "class RotMNIST(VisionDataset):\n",
    "    \"\"\"`RotMNIST`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where ``RotMNIST/processed/training.pt``\n",
    "            and  ``RotMNIST/processed/test.pt`` exist.\n",
    "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
    "            otherwise from ``test.pt``.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "\n",
    "    mirrors = [\n",
    "        'http://yann.lecun.com/exdb/mnist/',\n",
    "        'https://ossci-datasets.s3.amazonaws.com/mnist/',\n",
    "    ]\n",
    "\n",
    "    resources = [\n",
    "        (\"train-images-idx3-ubyte.gz\", \"f68b3c2dcbeaaa9fbdd348bbdeb94873\"),\n",
    "        (\"train-labels-idx1-ubyte.gz\", \"d53e105ee54ea40749a09fcbcd1e9432\"),\n",
    "        (\"t10k-images-idx3-ubyte.gz\", \"9fb629c4189551a2d022fa330f9573f3\"),\n",
    "        (\"t10k-labels-idx1-ubyte.gz\", \"ec29112dd5afa0611ce80d1b7f02629c\")\n",
    "    ]\n",
    "\n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "    classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four',\n",
    "               '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
    "\n",
    "    @property\n",
    "    def train_labels(self):\n",
    "        warnings.warn(\"train_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def test_labels(self):\n",
    "        warnings.warn(\"test_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def train_data(self):\n",
    "        warnings.warn(\"train_data has been renamed data\")\n",
    "        return self.data\n",
    "\n",
    "    @property\n",
    "    def test_data(self):\n",
    "        warnings.warn(\"test_data has been renamed data\")\n",
    "        return self.data\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            train: bool = True,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            download: bool = False,\n",
    "    ) -> None:\n",
    "        super(RotMNIST, self).__init__(root, transform=transform,\n",
    "                                    target_transform=target_transform)\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if self._check_legacy_exist():\n",
    "            self.data, self.targets = self._load_legacy_data()\n",
    "            return\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        self.data, self.targets = self._load_data()\n",
    "\n",
    "    def _check_legacy_exist(self):\n",
    "        processed_folder_exists = os.path.exists(self.processed_folder)\n",
    "        if not processed_folder_exists:\n",
    "            return False\n",
    "\n",
    "        return all(\n",
    "            check_integrity(os.path.join(self.processed_folder, file)) for file in (self.training_file, self.test_file)\n",
    "        )\n",
    "\n",
    "    def _load_legacy_data(self):\n",
    "        # This is for BC only. We no longer cache the data in a custom binary, but simply read from the raw data\n",
    "        # directly.\n",
    "        data_file = self.training_file if self.train else self.test_file\n",
    "        return torch.load(os.path.join(self.processed_folder, data_file))\n",
    "\n",
    "    def _load_data(self):\n",
    "        image_file = f\"{'train' if self.train else 't10k'}-images-idx3-ubyte\"\n",
    "        data =  mts.read_image_file(os.path.join(self.raw_folder, image_file))\n",
    "\n",
    "        label_file = f\"{'train' if self.train else 't10k'}-labels-idx1-ubyte\"\n",
    "        targets = mts.read_label_file(os.path.join(self.raw_folder, label_file))\n",
    "\n",
    "        return data, targets\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        # A note on the group p4m (rotation, flip, translation) and info on the conv.\n",
    "        # There are 4 possible rotations that can be done to form the rotation\n",
    "        # group of a set of data\n",
    "        # There are 2 possible flips than can be done that form the mirror \n",
    "        # group of a set of data\n",
    "        # \n",
    "        # Flipping both in X and in Y is the same as a rotation 180 degrees, and is\n",
    "        # therefore unecessary. Here is the procedure for generated the desired data:\n",
    "        # *     We decide the mirroring axis for the image (Image can stay the same,\n",
    "        # flip horiz. or flip vertically)\n",
    "        # *     We decide number of 90 degree rotations to apply to image\n",
    "\n",
    "        # Preform random rotation and mirroring for img\n",
    "        # Preform flip\n",
    "        img = torch.flip(img, [random.randint(-1, 1)])\n",
    "\n",
    "        # Preform random rotation (90 increments)\n",
    "        img = torch.rot90(img, random.randint(-1, 2), [-1, 1])\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    @property\n",
    "    def raw_folder(self) -> str:\n",
    "        return os.path.join(self.root, self.__class__.__name__, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_folder(self) -> str:\n",
    "        return os.path.join(self.root, self.__class__.__name__, 'processed')\n",
    "\n",
    "    @property\n",
    "    def class_to_idx(self) -> Dict[str, int]:\n",
    "        return {_class: i for i, _class in enumerate(self.classes)}\n",
    "\n",
    "    def _check_exists(self) -> bool:\n",
    "        return all(\n",
    "            check_integrity(os.path.join(self.raw_folder, os.path.splitext(os.path.basename(url))[0]))\n",
    "            for url, _ in self.resources\n",
    "        )\n",
    "\n",
    "    def download(self) -> None:\n",
    "        \"\"\"Download the MNIST data if it doesn't exist already.\"\"\"\n",
    "\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        os.makedirs(self.raw_folder, exist_ok=True)\n",
    "\n",
    "        # download files\n",
    "        for filename, md5 in self.resources:\n",
    "            for mirror in self.mirrors:\n",
    "                url = \"{}{}\".format(mirror, filename)\n",
    "                try:\n",
    "                    print(\"Downloading {}\".format(url))\n",
    "                    download_and_extract_archive(\n",
    "                        url, download_root=self.raw_folder,\n",
    "                        filename=filename,\n",
    "                        md5=md5\n",
    "                    )\n",
    "                except URLError as error:\n",
    "                    print(\n",
    "                        \"Failed to download (trying next):\\n{}\".format(error)\n",
    "                    )\n",
    "                    continue\n",
    "                finally:\n",
    "                    print()\n",
    "                break\n",
    "            else:\n",
    "                raise RuntimeError(\"Error downloading {}\".format(filename))\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"Split: {}\".format(\"Train\" if self.train is True else \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Jack\\anaconda3\\envs\\reu-code\\lib\\site-packages\\torchvision\\datasets\\mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:143.)\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 800x800 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"473.038125pt\" version=\"1.1\" viewBox=\"0 0 457.92 473.038125\" width=\"457.92pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-06-19T02:14:36.589386</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 473.038125 \r\nL 457.92 473.038125 \r\nL 457.92 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g clip-path=\"url(#p9bfc42d432)\">\r\n    <image height=\"443.52\" id=\"image8e3f0409a0\" transform=\"scale(1 -1)translate(0 -443.52)\" width=\"443.52\" x=\"7.2\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-22.318125\"/>\r\n   </g>\r\n   <g id=\"text_1\">\r\n    <!-- 9 -->\r\n    <g transform=\"translate(225.1425 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 10.984375 1.515625 \r\nL 10.984375 10.5 \r\nQ 14.703125 8.734375 18.5 7.8125 \r\nQ 22.3125 6.890625 25.984375 6.890625 \r\nQ 35.75 6.890625 40.890625 13.453125 \r\nQ 46.046875 20.015625 46.78125 33.40625 \r\nQ 43.953125 29.203125 39.59375 26.953125 \r\nQ 35.25 24.703125 29.984375 24.703125 \r\nQ 19.046875 24.703125 12.671875 31.3125 \r\nQ 6.296875 37.9375 6.296875 49.421875 \r\nQ 6.296875 60.640625 12.9375 67.421875 \r\nQ 19.578125 74.21875 30.609375 74.21875 \r\nQ 43.265625 74.21875 49.921875 64.515625 \r\nQ 56.59375 54.828125 56.59375 36.375 \r\nQ 56.59375 19.140625 48.40625 8.859375 \r\nQ 40.234375 -1.421875 26.421875 -1.421875 \r\nQ 22.703125 -1.421875 18.890625 -0.6875 \r\nQ 15.09375 0.046875 10.984375 1.515625 \r\nz\r\nM 30.609375 32.421875 \r\nQ 37.25 32.421875 41.125 36.953125 \r\nQ 45.015625 41.5 45.015625 49.421875 \r\nQ 45.015625 57.28125 41.125 61.84375 \r\nQ 37.25 66.40625 30.609375 66.40625 \r\nQ 23.96875 66.40625 20.09375 61.84375 \r\nQ 16.21875 57.28125 16.21875 49.421875 \r\nQ 16.21875 41.5 20.09375 36.953125 \r\nQ 23.96875 32.421875 30.609375 32.421875 \r\nz\r\n\" id=\"DejaVuSans-57\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-57\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p9bfc42d432\">\r\n   <rect height=\"443.52\" width=\"443.52\" x=\"7.2\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnwAAAKQCAYAAAAFa6evAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAA9hAAAPYQGoP6dpAAATA0lEQVR4nO3dX2gX9L/H8ffsqyNzUDdCLMxMImZSQVEXOSsJuwm6CYLuIjGoIMILL6IuuvEy6d9lwQjqIi+6EIKoi7BML3S5jVwlmX9q+adEzW3avufq/AgOB/bDz/rqy8fjcnx58b5xPveBaV+32+0WAACxFvX6AAAAFpbgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADrgl79uypjRs31sDAQC1btqweeeSR2rVrV6/PAvhXCD4g3t69e2t4eLguXLhQIyMjNTIyUtPT07Vhw4b65ptven0ewILr83/pAukef/zx2r9/fx06dKiWLl1aVVVnz56tVatW1R133OGlD4jnhQ+It2vXrnr44Yf/E3tVVQMDAzU8PFxff/11/frrrz28DmDhCT4g3uzsbPX39/+fr//v1w4cOPBvnwTwrxJ8QLyhoaHavXt3zc3N/edrly5dqm+//baqqk6dOtWr0wD+FYIPiPfSSy/V5ORkvfjii3Xs2LE6cuRIPf/883X48OGqqlq0yLdCIJvvckC8Z599trZt21YjIyN1yy231IoVK2piYqK2bNlSVVWDg4M9vhBgYfktXeCaMTMzUz/88EMNDAzUrbfeWps3b64PP/ywTpw4Uddff32vzwNYMJ1eHwDwb+nv76+77rqrqqp++eWX+vjjj2vTpk1iD4jnhQ+INzY2Vp988kndd9991d/fX6Ojo7Vt27ZauXJlffnll7Vs2bJenwiwoAQfEG9ycrI2bdpUY2Njde7cuVqxYkU9/fTTtXXr1rrhhht6fR7AghN8AADh/JYuAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhJv3f63W19e3kHcAAPBfmu8/p+yFDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCdXp9AFwNli1b1nRvaGio2dbg4GCzrdnZ2WZbU1NTzbaqqsbGxpptzczMNNvqdrvNtgAWihc+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACBcp9cHwNVgzZo1TfdeffXVZlsbN25stjU1NdVs67vvvmu2VVX15ptvNtvat29fs60//vij2dbff//dbAvgn7zwAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQLhOrw+Aq8H69eub7g0NDTXbGh8fb7Y1NjbWbGvdunXNtqqqduzY0Wxr69atzbY++uijZlunT59utlVV1e12m+4BVy8vfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAuE6vD4CrwalTp5runT9/vtnWoUOHmm298847zbY++OCDZltVVa+88kqzrZdffrnZ1vj4eLOt3bt3N9uqqpqenm66B1y9vPABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOE6vT4Argb79u1ruvfzzz8327rzzjubbd17773Ntnbs2NFsq6rq888/b7Y1PDzcbGvz5s3Ntk6ePNlsq6pqbGys6R5w9fLCBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCE6/T6ALgaTE5ONt0bHR1ttnX33Xc323rqqaeabV28eLHZVlVVp9Pu29WSJUuabS1fvrzZ1tKlS5ttAfyTFz4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAI1+n1AXA1OHfuXNO9PXv2NNt66KGHmm3df//9zbZuuummZltVVTMzM822lixZ0mzr7NmzzbZmZ2ebbQH8kxc+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACBcp9cHwLXoyJEjzbaOHj3abOvBBx9stnXdddc126qqmpiYaLa1Z8+eK3Jramqq2RbAP3nhAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCdXp9AHDlOHPmTLOtL774otlWVdUbb7zRbOv06dPNtrrdbrMtgIXihQ8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACNfp9QFApkuXLjXdO3XqVNM9gGuJFz4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcJ1eHwDXooMHDzbb+v7775ttPfPMM822hoeHm21VVS1fvrzZ1smTJ5ttzc3NNdsCWChe+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcJ1eHwDXogsXLjTbOnbsWLOt33//vdnWzTff3GyrqmrdunXNtj799NNmW3Nzc822ABaKFz4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAI1+n1AcDlmZqaarb1008/NdsaGhpqtlVV9cADDzTb2rlzZ7OtixcvNtsCWChe+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcJ1eHwBcnhMnTjTbOnToULOte+65p9lWVdXq1aubbS1a5Gdd4Nriux4AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEK7T6wOAy3P8+PFmW+Pj4822nnzyyWZbVVW33357s621a9c229q/f3+zrenp6WZbAP/khQ8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACNfp9QHA5Tl+/Hizrd27dzfbOnLkSLOtqqo1a9Y023ruueeabb3++uvNto4dO9Zs60o2MDDQbGtwcLDZVlXVX3/91Wzr6NGjzbbm5uaabXFt8sIHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4Tq9PgC4ckxOTjbbev/995ttVVVt27at2dbg4GCzrf7+/mZbfX19zbaqqrrdbrOtRYvavQ+sX7++2db27dubbVVVffXVV822XnjhhWZb58+fb7bFtckLHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQrtPrA4Arx59//tlsa3R0tNlWVdXJkyebbT322GPNtlavXt1s6/jx4822qqqmp6ebba1atarZ1rp165pt3Xjjjc22qqp+++23Zltzc3PNtuByeeEDAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMJ1en0AcOWYnp5utnXgwIFmW1VVb731VrOt1157rdnWli1bmm2dPn262VZV1cGDB5ttbdiwodnWE0880WxrYmKi2VZV1fbt25ttzczMNNuCy+WFDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAI19ftdrvz+mBf30LfAgRZvHhx073bbrut2dZ7773XbGvt2rXNtn788cdmW1VVZ86caba1cuXKZlvz/GtnXt59991mW1VVb7/9dtM9WGjz/fPkhQ8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADCCT4AgHCCDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACCc4AMACCf4AADC9XW73e68PtjXt9C3APy/Fi9e3Gzr0Ucfbba1adOmZlsbNmxotlVVtWhRu5/px8fHm23t3Lmz2dbIyEizraqqw4cPN92DhTbPjPPCBwCQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCE6/T6AID5uHjxYrOtzz77rNnW7Oxss63R0dFmW1VVixa1+5l+YmKi2dbevXubbR0+fLjZFiTzwgcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhOvrdrvdeX2wr2+hbwEA4L8wz4zzwgcAkE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQTvABAIQTfAAA4QQfAEA4wQcAEE7wAQCEE3wAAOEEHwBAOMEHABBO8AEAhBN8AADhBB8AQDjBBwAQrjPfD3a73YW8AwCABeKFDwAgnOADAAgn+AAAwgk+AIBwgg8AIJzgAwAIJ/gAAMIJPgCAcIIPACDc/wAT0A1CQ8dHMQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "### Instatiate RotMNIST and verify behaviour\n",
    "\n",
    "rot_dataset = RotMNIST(\n",
    "    root = 'data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.Resize(32), torchvision.transforms.ToTensor()]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Show random index of dataset (See FashionMNIST ipynb for details)\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "sample_idx = torch.randint(len(rot_dataset), size=(1,)).item()\n",
    "img, label = rot_dataset[sample_idx]\n",
    "plt.title(label)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "### Testing code for image manipulation\n",
    "# flip_int = [random.randint(0, 1)]\n",
    "# flip_int2 = [random.randint(-1, 0)]\n",
    "# img = torch.flip(img, flip_int)\n",
    "# img = torch.flip(img, flip_int2)\n",
    "# print(flip_int)\n",
    "# print(flip_int2)\n",
    "# plt.title(label)\n",
    "# plt.axis(\"off\")\n",
    "# plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5fb7fdfbbe8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### Instantiate dataloader for RotMNIST and get batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrot_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Feature batch shape: {train_features.size()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "### Instantiate dataloader for RotMNIST and get batches\n",
    "\n",
    "train_dataloader = DataLoader(rot_dataset, batch_size=64, shuffle=True)\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "img.shape\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-cb5020c4f882>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# NOTE: (use different pooling)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "### Basic NN architecture to build off of for g-conv \n",
    "# NOTE: (use different pooling)\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = C1\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  }
 ]
}